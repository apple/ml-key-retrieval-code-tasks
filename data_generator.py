#
# For licensing see accompanying LICENSE file.
# Copyright (C) 2024 Apple Inc. All Rights Reserved.
#

"""
Generate a set of experiments with synthetic code on mulit-step retrieval task.
"""

import gzip
import hashlib
import importlib
import json
import random
import typing
from dataclasses import asdict, dataclass
from io import TextIOBase, TextIOWrapper
from itertools import combinations, permutations
from typing import Annotated, Iterable, Optional, Union

import click
from datasets import load_dataset
from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast


REGEX_STRING = "^[ \t]*(['\"]|$)"
REGEX_INTEGER = "^[ \t]*([0-9]|$)"


@dataclass
class Configuration:
    """Data generation configuration."""

    model_name: Annotated[
        str,
        click.option("--model_name", required=True, help="HuggingFace model name"),
    ]

    variant: Annotated[
        str,
        click.option(
            "--variant",
            default="one-step",
            help="The task variant (one-step, two-step, three-step, concatenation).",
        ),
    ]

    return_type: Annotated[
        str,
        click.option(
            "--return_type",
            default="string",
            help="The return value type (string, integer).",
        ),
    ]

    return_length: Annotated[
        int,
        click.option(
            "--return_length",
            type=int,
            default=10,
            help="The max number of characters in the return value itself (not including quotes for strings).",
        ),
    ]

    function_name: Annotated[
        str,
        click.option(
            "--function_name",
            default="random",
            help="Function name (random, fixed).",
        ),
    ]

    function_name_part_length: Annotated[
        int,
        click.option(
            "--function_name_part_length",
            type=int,
            default=6,
            help="Part length for random function names. Function names are generated by concatenating multiple parts with underscore, each with length equal to function_name_part_length.",
        ),
    ]

    function_name_min_parts: Annotated[
        int,
        click.option(
            "--function_name_min_parts",
            type=int,
            default=2,
            help="Min number of parts for random function names. The actual number of parts is picked from a uniform distribution between min and max number of parts.",
        ),
    ]

    function_name_max_parts: Annotated[
        int,
        click.option(
            "--function_name_max_parts",
            type=int,
            default=3,
            help="Max number of parts for random function names.",
        ),
    ]

    call_graph_comment_type: Annotated[
        str,
        click.option(
            "--call_graph_comment_type",
            default="calls,called_by",
            help="(krfix) Comma-separated list of call graph comment types. Valid values are `calls` and `called_by`.",
        ),
    ]

    call_graph_template_variant: Annotated[
        str,
        click.option(
            "--call_graph_template_variant",
            default="calls_called_by",
            help="(krfix) Call graph template variant, `calls_called_by` or `function_names_only`.",
        ),
    ]

    call_graph_comment_position: Annotated[
        str,
        click.option(
            "--call_graph_comment_position",
            default="before",
            help="(krfix_one_hop) Call graph comment position, before or after.",
        ),
    ]

    num_key_functions: Annotated[
        int,
        click.option(
            "--num_key_functions",
            type=int,
            default=1,
            help="The number of key functions.",
        ),
    ]

    max_prompt_tokens: Annotated[
        int,
        click.option(
            "--max_prompt_tokens",
            type=int,
            default=2000,
            help="The max number of tokens in the prompt.",
        ),
    ]

    max_humaneval_snippets: Annotated[
        int,
        click.option(
            "--max_humaneval_snippets",
            type=int,
            default=10000,
            help="The max number of code snippets from humaneval.",
        ),
    ]

    num_distractors: Annotated[
        int,
        click.option(
            "--num_distractors",
            type=int,
            default=0,
            help="The number of distractors. These will be sampled by generating a number of tasks to reach this given number of snippets (one task may have multiple snippets).",
        ),
    ]

    max_position_combinations: Annotated[
        Optional[int],
        click.option(
            "--max_position_combinations",
            type=int,
            help="The max number of task segment position combinations. If the number of possible combinations is greater than this, random sample to this number.",
        ),
    ]

    humaneval_min_length: Annotated[
        int,
        click.option(
            "--humaneval_min_length",
            type=int,
            default=250,
            help="Filter humaneval data with the min string length.",
        ),
    ]

    humaneval_max_length: Annotated[
        int,
        click.option(
            "--humaneval_max_length",
            type=int,
            default=500,
            help="Filter humaneval data with the max string length.",
        ),
    ]

    seed: Annotated[
        int,
        click.option(
            "--seed",
            type=int,
            default=100,
            help="The random seed.",
        ),
    ]

    @classmethod
    def define_options(cls, fn):
        """A decorator that defines click command line options for fields in this configuration class."""
        r = fn
        for _, annotation in reversed(cls.__annotations__.items()):
            if typing.get_origin(annotation) is Annotated:
                r = typing.get_args(annotation)[1](r)
        return r

    def default_file_suffix(self):
        """Default file suffix."""
        return "_".join(
            map(
                lambda t: f"{t[0]}={t[1]}",
                [
                    ("nkf", self.num_key_functions),
                    ("rt", self.return_type),
                    ("rl", self.return_length),
                    ("maxpt", self.max_prompt_tokens),
                    ("maxhs", self.max_humaneval_snippets),
                    ("nd", self.num_distractors),
                    ("maxpc", self.max_position_combinations),
                    ("hminl", self.humaneval_min_length),
                    ("hmaxl", self.humaneval_max_length),
                    ("seed", self.seed),
                ],
            )
        )


def encode_without_leading_space(
    text: str, tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
) -> list[int]:
    """
    Returns the token ids for the given string,
    without adding a leading space in the first token, also without start token.
    """
    newline_ids = tokenizer.encode("\n", add_special_tokens=False)
    text_ids = tokenizer.encode("\n" + text, add_special_tokens=False)
    assert newline_ids == text_ids[: len(newline_ids)]
    return text_ids[len(newline_ids) :]


def load_humaneval(min_length: int = 0, max_length: int = 1000000):
    """Load top-level functions from the HumanEval dataset, and filter by string length to [min_length, max_length]."""
    ds = load_dataset("openai_humaneval", split="test")
    functions = [
        x["prompt"] + x["canonical_solution"]  # type: ignore
        for x in ds
        if x["prompt"].startswith("\ndef")  # type: ignore
    ]
    result: list[str] = []
    for f in functions:
        # Filter by string length.
        if len(f) < min_length or len(f) > max_length:
            continue
        # Make sure we have the right format.
        assert f.startswith("\ndef") and f.endswith("\n")
        result.append(f.strip())
    return result


def find_index_of_subarray(array: list[int], subarray: list[int]) -> int:
    """Find the index of the subarray from the array."""
    for i in range(len(array)):
        if array[i : i + len(subarray)] == subarray:
            return i
    return -1


def find_function_token_range(
    token_ids: list[int],
    function: str,
    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
) -> tuple[int, int]:
    """Find the token range the function from text (given as token ids)."""
    ids1 = encode_without_leading_space(function, tokenizer)
    ids2 = tokenizer.encode(function, add_special_tokens=False)
    for ids in [ids1, ids2]:
        start_index = find_index_of_subarray(token_ids, ids)
        if start_index < 0:
            continue
        end_index = start_index + len(ids)
        # Sanity check to make sure we slice the right subarray.
        assert tokenizer.decode(token_ids[start_index:end_index]) == function
        return start_index, end_index

    print(
        repr(function),
        tokenizer.convert_ids_to_tokens(token_ids),
        tokenizer.convert_ids_to_tokens(ids1),
        tokenizer.convert_ids_to_tokens(ids2),
    )
    raise ValueError("cannot find the function")


def find_string_index_range(text: str, function: str) -> tuple[int, int]:
    """Find the string index of the function from text, so that function == text[start:end]."""
    start = text.index(function)
    end = start + len(function)
    assert start >= 0
    assert text[start:end] == function
    return start, end


def get_last_line(text: str):
    """Get the last line from the string."""
    return text.split("\n")[-1]


def generate_key_retrieval_multistep_single(
    task_builder_class,
    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
    rng: random.Random,
    humaneval: list[str],
    cfg: Configuration,
) -> Iterable[dict]:
    """Generate a single multi-step key retrieval task with all of its permutations and insertions."""

    # Build the task.
    builder = task_builder_class(configuration=cfg, random_number_generator=rng)
    task_segments, generation_prompt, expected = builder.build(cfg.variant)

    # Prepare code snippets to insert in between context snippets.

    # a. from humaneval.
    humaneval_snippets: list[str] = humaneval.copy()
    rng.shuffle(humaneval_snippets)
    humaneval_snippets = humaneval_snippets[: cfg.max_humaneval_snippets]

    # b. from the same kind of task.
    synthetic_snippets: list[str] = []
    while len(synthetic_snippets) < cfg.num_distractors:
        segs, _, _ = builder.build(cfg.variant)
        synthetic_snippets += segs
    rng.shuffle(synthetic_snippets)
    synthetic_snippets = synthetic_snippets[: cfg.num_distractors]

    # c. limit the number of tokens to <= max_prompt_tokens.
    humaneval_count = len(humaneval_snippets)
    while humaneval_count > 0:
        text = "\n\n".join(
            task_segments
            + synthetic_snippets
            + humaneval_snippets[:humaneval_count]
            + [generation_prompt]
        )
        count = len(tokenizer.tokenize(text, add_special_tokens=True))
        count += 10  # be a bit conservative so we don't run out of tokens.
        if count > cfg.max_prompt_tokens:
            humaneval_count -= 1
        else:
            break

    # d. irrelevant snippets = humaneval snippets + synthetic snippets, shuffled.
    irrelevant_snippets = humaneval_snippets[:humaneval_count] + synthetic_snippets
    rng.shuffle(irrelevant_snippets)

    # Find all position combinations.
    position_combinations = list(
        combinations(
            range(len(irrelevant_snippets) + len(task_segments)),
            len(task_segments),
        )
    )
    # If the total number of combinations is greater than the max_insert_count_combinations, sample it.
    if (
        cfg.max_position_combinations is not None
        and cfg.max_position_combinations < len(position_combinations)
    ):
        position_combinations = rng.sample(
            position_combinations, cfg.max_position_combinations
        )
    for positions in position_combinations:
        # For each permutation of the context segments.
        for perm_segments_indices in permutations(range(len(task_segments))):
            perm_positions = [positions[i] for i in perm_segments_indices]

            prompt_parts = []
            irs_pos = 0
            for i in range(len(irrelevant_snippets) + len(task_segments)):
                if i in perm_positions:
                    prompt_parts.append(task_segments[perm_positions.index(i)])
                else:
                    prompt_parts.append(irrelevant_snippets[irs_pos])
                    irs_pos += 1
            assert irs_pos == len(irrelevant_snippets)
            prompt_parts.append(generation_prompt)

            prompt = "\n\n".join(prompt_parts)

            prompt_token_ids = tokenizer.encode(prompt, add_special_tokens=True)

            # Produce segment ranges.
            string_ranges = []
            token_ranges = []
            for seg in task_segments:
                token_range = find_function_token_range(
                    prompt_token_ids, seg, tokenizer
                )
                token_ranges.append(list(token_range))
                string_range = find_string_index_range(prompt, seg)
                string_ranges.append(list(string_range))
            metadata = {
                "expected": expected,
                "prompt_token_count": len(prompt_token_ids),
                "permutation": perm_segments_indices,
                "positions": perm_positions,
                "string_ranges": string_ranges,
                "token_ranges": token_ranges,
                "prompt_sha1": hashlib.sha1(prompt.encode("utf-8")).hexdigest(),
            }
            # Note: this regex should match any possible output prefix (even empty string), and reject impossible prefix.
            if cfg.return_type == "string":
                # allow: ``, ` `, ` "`, ` "abcd`, ` "abcd"`
                # reject: `abc`, ` abc()`
                force_decode_regex = REGEX_STRING
            else:
                # allow: ``, ` `, ` 3`, ` 352`
                # reject: `abc`, ` abc()`
                force_decode_regex = REGEX_INTEGER

            for key, value in asdict(cfg).items():
                metadata[key] = value
            yield {
                "prompt": prompt,
                "force_decode_regex": force_decode_regex,
                "generation_config": {"max_new_tokens": len(expected) + 1},
                "metadata": metadata,
            }


def generate_key_retrieval_multistep(
    task_builder_class,
    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
    cfg: Configuration,
):
    """Generate multi-step key retrieval experiment dataset."""
    rng = random.Random(cfg.seed)
    humaneval = load_humaneval(
        min_length=cfg.humaneval_min_length,
        max_length=cfg.humaneval_max_length,
    )
    for _ in range(cfg.num_key_functions):
        for x in generate_key_retrieval_multistep_single(
            task_builder_class=task_builder_class,
            tokenizer=tokenizer,
            rng=rng,
            humaneval=humaneval,
            cfg=cfg,
        ):
            yield x


def stream_into_json(stream: TextIOBase, iterator: Iterable):
    """Serialize items from the iterator into a JSON array and write into the stream."""
    count = 0
    stream.write("[\n")
    last_item = None
    for x in iterator:
        if last_item is not None:
            stream.write(json.dumps(last_item) + ",\n")
        last_item = x
        count += 1
    if last_item is not None:
        stream.write(json.dumps(last_item) + "\n")
    stream.write("]\n")
    return count


@click.command()
@click.argument("task_builder", type=str, required=True)
@Configuration.define_options
@click.option("--output", "-o", help="The output file.", type=str)
def cli(task_builder, output, **kwargs):
    """The command line entrypoint."""
    click.echo(f"Task builder: {task_builder}")

    task_builder_class = getattr(
        importlib.import_module(task_builder),
        "TaskBuilder",
    )

    click.echo(f"Task builder: {task_builder_class}")

    cfg = Configuration(**kwargs)
    click.echo(f"Configuration: {json.dumps(asdict(cfg), indent=2)}")

    if output is None:
        output = (
            "_".join(
                [
                    cfg.variant,
                    cfg.model_name.replace("/", "-"),
                    cfg.default_file_suffix(),
                ]
            )
            + ".json"
        )

    result = generate_key_retrieval_multistep(
        task_builder_class=task_builder_class,
        tokenizer=AutoTokenizer.from_pretrained(cfg.model_name),
        cfg=cfg,
    )

    if output.endswith(".gz"):
        with gzip.GzipFile(output, "w", mtime=0) as f:
            count = stream_into_json(TextIOWrapper(f, "utf-8"), result)
    else:
        with open(output, "w", encoding="utf-8") as f:
            count = stream_into_json(f, result)

    print(f"{count} prompts written into {output}")


if __name__ == "__main__":
    cli()
